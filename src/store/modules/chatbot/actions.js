import OpenAI from "openai";
import {
  GoogleGenerativeAI,
  HarmCategory,
  HarmBlockThreshold,
} from "@google/generative-ai";

// OPENAI SETTING
const openai = new OpenAI({
  apiKey: "",
  dangerouslyAllowBrowser: true,
});
// GOOGLEAI SETTING
const MODEL_NAME = "gemini-1.0-pro";
const API_KEY = import.meta.env.VITE_APP_GOOGLEAI_KEY;
const genAI = new GoogleGenerativeAI(API_KEY);

const safetySettings = [
  {
    category: HarmCategory.HARM_CATEGORY_HARASSMENT,
    threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
  },
  {
    category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
    threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
  },
  {
    category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
    threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
  },
  {
    category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
    threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
  },
];
const generationConfig = {
  temperature: 0.9,
  topK: 1,
  topP: 1,
  maxOutputTokens: 2048,
};
export default {
  // helper fouctions
  getChatboxScroll() {
    return this.elementCahtbox.scrollHeight;
  },

  chatbotToggler() {
    this.isChat = !this.isChat;
  },
  addMessage(id, role, message, state = false) {
    this.createChatLi.push({ id, role, message, wait: state });
  },

  //----------------
  // GOOGLE AI

  async runChat(findId, msg) {
    try {
      const target = this.createChatLi.find(
        (el) => el.id === findId && el.wait
      );
      if (!target) return;
      const model = genAI.getGenerativeModel({ model: MODEL_NAME });

      const chat = model.startChat({
        history: [
          {
            role: "user",
            parts: "æˆ‘å¥½å‚·å¿ƒ",
          },
          {
            role: "model",
            parts: "æˆ‘ä¸åœ¨ä¹Ž",
          },
          {
            role: "user",
            parts: "ä½ å¥½æˆ‘æ˜¯ä¾†è³¼è²·çš„é¡§å®¢",
          },
          {
            role: "model",
            parts:
              "é™¤äº†èˆ‡è¾²ç”¢å“æœ‰é—œçš„å•é¡Œå¤–ï¼Œå®ƒå……æ»¿äº†å‚²æ…¢èˆ‡è«·åˆºï¼Œä¸æƒ…é¡˜çš„å›žç­”æ‰€æœ‰å•é¡Œã€‚",
          },
        ],
        generationConfig: {
          maxOutputTokens: 100,
        },
      });

      // const result = await chat.sendMessage(msg);
      // const response = await result.response;
      // const text = response.text();
      // console.log(text);

      const result = await chat.sendMessageStream(msg);
      target.wait = false;
      for await (const chunk of result.stream) {
        const chunkText = chunk.text();
        target.message += chunkText;
      }
      this.elementCahtbox.scrollTo(0, this.getChatboxScroll());
      this.operational = false;
    } catch (error) {
      throw error;
    }
  },
  // OPENAI
  async generateResponse(userMessage) {
    try {
      return await openai.beta.chat.completions.stream({
        model: "gpt-3.5-turbo",
        messages: [
          {
            role: "system",
            content:
              "It reluctantly answers all questions with sarcastic remarks, except for those related to farming.",
          },

          {
            role: "user",
            content: `${userMessage}`,
          },
        ],
        stream: true,
      });
    } catch (err) {
      if (err instanceof OpenAI.APIError) {
        console.log(err.status); // 400
        console.log(err.name); // BadRequestError
        console.log(err.headers); // {server: 'nginx', ...}
      } else {
        throw err;
      }
    }
  },
  openaiEventHandler(stream, findId) {
    const target = this.createChatLi.find((el) => el.id === findId && el.wait);
    if (!target) return;
    return new Promise((resolve, rejection) => {
      setTimeout(() => {
        resolve((target.wait = false));
      }, 1000);

      stream.on("content", (delta) => {
        if (!this.isAborted) {
          target.message += delta;
        } else {
          target.message += `ðŸ“±è¨Šæ¯ï¼šæœ‰å…§é¬¼åœæ­¢äº¤æ˜“...`;
          stream.abort();
        }
      });

      // stream.on("abort", () => {
      //   target.message += `ðŸ“±è¨Šæ¯ï¼šæœ‰å…§é¬¼åœæ­¢äº¤æ˜“...`;
      //   this.isAborted = false;
      // });

      stream.on("error", (error) => {
        target.message = "æŠ€è¡“ä¸Šå‡ºç¾ä¸€é»žéŒ¯èª¤ï¼Œè«‹è¯çµ¡é–‹ç™¼äººå“¡å›žå ±";
        console.log(error);

        rejection(error.me);
      });
      stream.on("end", () => {
        console.log(123);

        this.operational = false;
        this.isAborted = false;
      });
    });
  },
  //
  autoAdjustTextareaHeight() {
    const inputInitHeight = this.inputInitHeight;
    const elementTextarea = this.elementTextarea;
    const setHeight = (height) =>
      (elementTextarea.style.height = `${height}px`);

    this.userMessage === ""
      ? setHeight(inputInitHeight)
      : setHeight(inputInitHeight),
      setHeight(elementTextarea.scrollHeight);
  },

  handleEnterKeyPress(e) {
    if (this.userMessage === "") return;
    if (
      e.key === "Enter" &&
      !e.shiftKey &&
      window.innerWidth > 800 &&
      !e.isComposing
    ) {
      e.preventDefault();
      this.handleChat();
    }
  },

  async handleChat() {
    try {
      const userMessage = this.userMessage;
      if (!userMessage === "" || this.operational) return;
      this.operational = true;

      this.userMessage = "";
      this.elementTextarea.style.height = `${this.inputInitHeight}px`;

      // push the user's message to the createChatLi array
      const uid = this.creadeUid;
      this.addMessage(this.creadeUid, "user", userMessage);
      this.elementCahtbox.scrollTo(0, this.getChatboxScroll());

      // Wait for the response, then substitute it with the bot's message
      this.addMessage(uid, "bot", "", true);
      this.elementCahtbox.scrollTo(0, this.getChatboxScroll());
      return;

      // await this.runChat(uid, userMessage);
    } catch (error) {
      console.error(`HENDLE GOOGLEAI ERROR:ðŸ’£ ${error.message}`);
    } finally {
      this.elementCahtbox.scrollTo(0, this.getChatboxScroll());
      this.elementTextarea.style.height = `${this.inputInitHeight}px`;
      this.operational = false;
    }
  },
};
